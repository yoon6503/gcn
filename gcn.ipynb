{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "numalph = 0\n",
    "#unzip ~/PycharmProjects/test\n",
    "\n",
    "#train data\n",
    "chardict = {} #[alphabet: di ct, ...]\n",
    "# charname = [] #list of alphabet\n",
    "\n",
    "root = os.path.abspath(\"/home/yoon1524/PycharmProjects/test/images_background\")\n",
    "charname = os.listdir(root) #English, Korean .... string type\n",
    "for alphname in charname: #create train set\n",
    "    charroot = os.path.join(root, alphname)\n",
    "    charroot = os.path.abspath(charroot)\n",
    "    chartype = os.listdir(charroot) #character01, ...\n",
    "    typedict = {}\n",
    "    # typedictint = {} #int version of typedict\n",
    "    numtype = 0\n",
    "    for dirname in chartype:\n",
    "        typedict[dirname] = os.listdir(os.path.join(charroot, dirname))\n",
    "        # typedictint[numtype] = os.listdir(os.path.join(charroot, dirname))\n",
    "        numtype += 1\n",
    "    chardict[alphname] = typedict\n",
    "    numalph += 1\n",
    "\n",
    "class Omniglot(Dataset):\n",
    "\n",
    "    #datafile: ex) English, a\n",
    "    def __init__(self, data_file, sp_data_file, root_dir, transform = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data_file = data_file\n",
    "        self.sp_data_file = sp_data_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sp_data_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx : num of char type (range(20))\n",
    "        image_path = os.path.join(self.root_dir, os.listdir(self.root_dir)[idx])\n",
    "        dirlist = self.root_dir.split(os.path.sep)\n",
    "        alphabet = dirlist[-2]\n",
    "        character = dirlist[-1]\n",
    "        img = Image.open(image_path).resize((28, 28))\n",
    "        img = img.convert('L')\n",
    "        x = torch.from_numpy(np.asarray(img))\n",
    "        y = torch.zeros((1,2))\n",
    "        for i, (lang, chard) in enumerate(self.data_file.items()):\n",
    "           if lang == alphabet:\n",
    "                y[0,0] = i\n",
    "                for j, char in enumerate(chard):\n",
    "                    if char == character:\n",
    "                        y[0,1] = j\n",
    "                        break\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "def datasampler(omnig, num_sample):\n",
    "    idxlist = np.random.choice(omnig.__len__(), num_sample, replace=False)\n",
    "    x_sample = torch.zeros((num_sample,)+omnig.__getitem__(0)[0].shape)\n",
    "    y_sample = torch.zeros((num_sample, 1, 2))\n",
    "    i = 0\n",
    "    for idx in idxlist:\n",
    "        x_sample[i:], y_sample[i:] = omnig.__getitem__(idx)\n",
    "        i += 1\n",
    "    return x_sample, y_sample\n",
    "\n",
    "def randomclass(dataset, num_class):\n",
    "    random_class = []\n",
    "    for i in range(num_class):\n",
    "        alphabet = random.choice(list(dataset.items()))\n",
    "        _character = random.choice(list(alphabet[1].items()))\n",
    "        class_tuple = (alphabet[0], _character[0])\n",
    "        while class_tuple in random_class:\n",
    "            alphabet = random.choice(list(dataset.items()))\n",
    "            _character = random.choice(list(alphabet[1].items()))\n",
    "            class_tuple = (alphabet[0], _character[0])\n",
    "        random_class.append(class_tuple)\n",
    "    return random_class\n",
    "\n",
    "random_class = randomclass(chardict, 1)\n",
    "root_dir = os.path.join(root, random_class[0][0], random_class[0][1])\n",
    "sp_data_file = os.listdir(root_dir)\n",
    "\n",
    "dataset = Omniglot(chardict, sp_data_file, root_dir)\n",
    "x_sample, y_sample = datasampler(dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        return output\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, idx_train = load_data()\n",
    "\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# Train model\n",
    "for i in range(epoch):\n",
    "    train()\n",
    "print(\"Optimization Finished!\")"
   ]
  }
 ]
}